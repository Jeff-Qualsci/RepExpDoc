---
title: "Replicate Experiment"
format: 
  html: 
    toc: true
    toc-depth: 3
   
    
execute:
  echo: false
  messages:  false
  warning: false
  error: false
---

# Replicate-Experiment

## Introduction

Statistical process control (SPC) is the use of statistical methods to monitor and control the quality of processes. Though the tools were initially developed and used for manufacturing, they have found applications in many other areas where outcomes can be measured and quality is valued, including scientific laboratories. Some of the most common SPC tools include Run Charts, Control Charts, Experimental Design, and Replicate Experiment.

The Replicate-Experiment^1a Eastwood 2006^ is based on Bland-Altman difference analysis^[1](https://www-users.york.ac.uk/~mb55/meas/ba.pdf)^ between two sets of measurements and is used to assess either **repeatability** or **reproducibility**. These terms have specific meanings in SPC. **Repeatability** is the ability to produce comparable results within a group of conditions (e.g. people, intstruments, reagents, ...), while **reproducibility** refers to the ability to replicate results across different conditions (e.g. between two assayers or pieces of equipment). Initially repeatability should be demonstrated by having a scientist test the same set of samples with 2 independently prepared sets of working reagents/cells (note: these experiments can be performed at the same time or on different days) on the same set of equipment. Once repeatability has been established, reproducibility can be determined between different assayers, pieces of equipment, or lots of reagents or cells.

## Rationale

Replicate-Experiment studies^2^ are used to formally evaluate the *within-run* assay variability and formally compare a new assay to the existing (old) assay. They also allow a preliminary assessment of the *overall* or *between-run* assay variability, but two runs are not enough to adequately assess overall variability. Post-Production monitoring, such as Retrospective MSR^[3](https://www.ncbi.nlm.nih.gov/books/NBK169432/)^ analysis and Control Charts are used to formally evaluate the overall variability in the assay. Note that the Replicate-Experiment study is a diagnostic and decision tool used to establish that the assay is ready to go into production by showing that the endpoints of the assay are repeatable over a range of values. It is not intended as a substitute for post-production monitoring or to provide an estimate of the overall Minimum Significant Ratio (MSR).

It may seem counter-intuitive to call the differences between two independent assay runs as *within-run* variability. However, the terminology results from how assay runs are defined. Experimental variation is categorized into two distinct components: *between-run* and *within-run* sources. Consider the following examples:

-   If there is variation in the concentrations of buffer components between 2 runs, then the assay results could be affected. However, assuming that the same buffer is used with all compounds within one run, each compound will be equally affected and so the difference will only show up when comparing one run to another run, i.e. in two runs, one run will appear higher on average than the other run. This variation is called *between-run* variation.

-   If the concentration of a compound in the stock plate varies from the target concentration then all wells where that compound is used will be affected. However, wells used to test other compounds will be unaffected. This type of variation is called *within-run* as the source of variation affects different compounds in the same run differently.

-   Some sources of variability affect both within- and between-run variation. For example, if assay cells are plated and then incubated for 24-72 hours to achieve a target cell density taking into account the doubling time of the cells. If the doubling time equals the incubation time, and the target density is 30,000 cells/well, then 15,000 cells/well are plated. But even if exactly 15,000 cells are placed in each well there won't be exactly 30,000 cells in each well after 24 hours. Some will be lower and some will be higher than the target. These differences are *within-run* as not all wells are equally affected. But also suppose in a particular run only 13,000 cells are initially plated. Then the wells will on average have fewer than 30,000 cells after 24 hours, and since all cells are affected this is *between-run* variation. Thus cell density has both *within*- and *between*-run sources of variation.

The total variation is the sum of both sources of variation. When comparing two compounds across runs, one must take into account both the *within-run* and *between-run* sources of variation. But when comparing two compounds in the same run, one must only take into account the *within-run* sources, since, by definition, the *between-run* sources affect both compounds equally.

In a Replicate-Experiment study the *between-run* sources of variation cause one run to be on average higher than the other run. However, it would be very unlikely that the difference between the two runs were exactly the same for every compound in the study. These individual compound "differences from the average difference" are caused by the *within-run* sources of variation. The higher the within-run variability the greater the individual compound variation in the assay runs.

**The analysis approach used in the Replicate-Experiment study is to estimate and factor out between-run variability, and then estimate the magnitude of within-run variability.**

## Experimental Procedure

The Replicate-Experiment is intended to be easy to execute with a modest resource commitment. Most executions can be performed with 2-4 assay plates. It is most commonly run in the potency mode, though it can be run in efficacy mode to gain a better understanding of assay variability across the dynamic range of the assay or facilitate the interpretation of screening results.

The potency mode is ideally run with 20-30 active compounds with a broad range of potencies and the potencies should be well spaced across the range of potencies. If this number of active compounds is not available, then it can be run with a smaller set of compounds in replicate, with each replicate treated as an independent sample (e.g. 5 compounds with 5 seperate replicate dilutions).

In the efficacy mode, it is particularly important to have samples where the activity spans the dynamic range of the assay. Often the variability of measurements will not be constant across the dynamic range of the assay. For this reason, efficacy studies should not be conducted with random screening plates, since most compounds will be inactive and could skew the assessment. It may be simpler to use a small number of active compounds in a. dilution series, as if for a potency determination, but treat each dilution as an independent sample for the efficacy analysis. This will ensure that the data cover the entire dynamic range of the assay with just a few compounds.

Initially **repeatability** should be demonstrated with identical compounds tested with 2 independently prepared sets of reagents/cells. Once **repeatability** has been demonstrated for a protocol, it is ready for routine testing. The assay should be monitored to ensure it behaves as validated. This can include any or all of the following methods:

-   Control charting^Control Charts Ref.^ reference compound(s)

-   Retrospective MSR studies^reference^

-   periodic retests if the samples used in previous replicate experiments to compare to previous data.

The replicate experiment is also used to validate minor assay changes such as new assayers, equipment substitutions, or changes to lots of reagents or cells. In this case the data from identical samples is compared in the current and new formats. Historical data can be used for the current format, as long as it comes from a single experiment.

## Data Analysis {#sec-data_analysis}

The statistical analysis assumes that any measurement errors are normally distributed. While this is true for efficacy data, potency data is log-normal. This means that potency data must first be transformed to their log~10~ values before the analysis. After that transformation the analysis methods are the same:

1.  For each pair of compound measurements, calculate the mean = (meas1 + meas2)/2 and the difference = (meas1 - meas2). *Note when these values are transformed back to the linear scale for potency data they will generate the geometric mean and the ratio, since log(meas1) - log(meas2) = log(meas1/meas2).*

2.  Calculate the mean ($\bar{d}$) and standard deviation (sd) for the set of the difference values.

3.  Calculate the difference limits $DLs = \bar{d}\pm2sd/\sqrt{n}$ Where n is the number of compounds tested. This is the 95% confidence interval for the mean difference.

4.  Calculate limits of aggrement $LSAs = \bar{d}\pm2sd$ Most of the compound differences (\~95%) should fall within these limits.

5.  Samples outside the aggrement limits are flagged as outliers.

6.  Calculate the Minimum Significant Difference $MSD = 2sd$ This is the minimum difference between two compounds that is statistically significant.

7.  For potency data all statistics are transformed back to linear scale and differences become ratios (e.g. Minimum Significant Ratio $MSR = 10^{MSD}$).

If the variation between measurements of different samples is consistent and normally distributed, then the population of differences would be similar to a population of residuals and demonstrate a normal distribution, centered on 0, if there is no systematic bias between the two measurements.

![Difference Distribution for Replicate Experiment.](Figs/DiffPlot.png){#fig-diffstats}

Figure 1. illustrates that distribution of difference values along with the associated analysis statistics. The difference limits are derived from the standard error of the mean (SEM) and represent a 95% CI around the mean difference. If the difference limits include 0, then there is no discernable systematic bias between the first and second measurement. The limits of statistical agreement (LSA) are the mean $\pm$ 2 standard deviations (95% CI) (green area) . While the MSD is equal to 2 standard deviations, so two measurements (either of the same sample or between 2 different samples) which differ by more than the MSD are considered to be statistically different. Finally those samples whose differences are more than 2 standard deviations from the mean (p \< 0.05) are flagged as potential outliers. If samples are flagged, then they may be excluded from the data set and the analysis repeated. **Note: As the number of samples increases, a few samples may fall outside the LSA due to random variation, especially if they are just outside of the LSA. If outliers are to be excluded, start with the most extreme differences. Only obvious outliers or those with an assignable cause should be removed.**

Fortunately all of this analysis can be done with the [Replicate-Experiment web tool](https://agm.ncats.nih.gov/app/ "https://agm.ncats.nih.gov/app/"). Just create a .csv file of your data in any spreadsheet program and upload it to the tool for analysis.

## Efficacy

Efficacy measurements can be made with raw data from a detector or data which has been normalized (e.g. % Activity) using plate controls. Normalized data is preferred, since it provides biological context and facilitates comparisons across the lifetime of an assay. ***Note all the example data sets are % activity, where the vehicle control is set to 0.***

A replicate-experiment can be used to determine the minimum difference in measured activities for 2 samples to be statistically different ( $p \leq 0.05$). This is known as the **Minimum Significant Difference (MSD)**. The following scenario illustrates how MSD can be determined using just 2 384-well assay plates. Each plate contains the same 320 samples with activities that cover the full dynamic range of the assay and 64 control wells for the normalization. The plates are tested with independently prepared reagents/cells.

Here are the data from the first 6 samples. The data is simply the sample identifiers (numeric or character) and the measured activity values for the 2 experiments. This is all that is needed to upload for analysis.

```{r efficacy320data, echo=FALSE}
#| label: tbl-efficacy
#| tbl-cap: "Sample efficacy data."

library(tidyverse)
library(knitr)

MSD20Data <- read_csv('Data/MSD20Data320.csv')

kable(head(MSD20Data))


```

The first step of the analysis is to simply compute the average activity and the difference (Exp1 - Exp2) for each sample. The mean represents an estimate of the true activity for a sample, while the difference provides an estimate of the variability in the measurements.

```{r efficacy320CalcData, echo=FALSE}

MSD20CalcData <- read_csv('Webtool Output/MSD20_JRW/MSD20Data320_EFF_OutputData_tbl.csv') %>% 
  select(Sample, Exp1, Exp2, MeasMean, MeasDiff, Class) %>% 
  rename(Mean = MeasMean, Difference = MeasDiff) %>% 
  mutate(Sample = as.character(Sample),
         across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE)))

  

knitr::kable(head(MSD20CalcData), 'pipe')
```

***Note. While the input data may contain any number of digits after the decimal point and are used for the analysis, Calculated values and statistics are displayed to 3 significant digits***^4^***.***

![Mean-Difference plot of efficacy data.](Webtool%20Output/MSD20_JRW/MSD20MD.png){#fig-effMDplot}

The primary output of the analysis is the Mean Difference plot. The mean value of each data pair is plotted on the x-axis while the difference between the points is represented on the y-axis. Plotting the mean on the x-axis and difference on the y-axis provides an overview of the variability in the data across the activity range of the assay. This plot is similar to a residuals plot. If the data were identical and the variability was simply random, then the data points should be centered around 0 with about half of the data on either side of the center. The statistics in the plot are based on the distribution in @fig-diffstats.

The blue lines show the mean difference (solid) and the difference limits (dashed) as well as a black reference line at 0. The 0 reference line is the expected difference, if the variability is random. The difference limits represent a 95% CI around the mean difference. If this includes 0, then the variability in the data is random with no discernable systematic bias between the two runs. If the reference line does not fall within the difference limits, this suggests a difference between the runs due to a cause (e.g. reagents, equipment, samples, environment, ...).

The plot also includes agreement limits (95% CI) around the mean difference. The distance between the agreement limits is 2 standard deviations from the mean. This is also the minimum significant difference (MSD). Any two efficacy measurements, both within and between samples, are considered to be significantly different if the difference is greater than the MSD. These statistics are derived from the difference values as described in @sec-data_analysis.

![Histogram of Difference Values.](Reports/MSD20/EfficacyHist.png){#fig-diffhist}

```{r efficacy320Stats, echo=FALSE}

MSD20Stats <- read_csv('Webtool Output/MSD20_JRW/MSD20Data320_EFF_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE)))

knitr::kable(MSD20Stats, 'pipe')
```

@fig-diffhist shows the distribution of the difference values. This is not part of the replicate experiment report, but may help to illustrate the analysis. Samples with differences outside of the agreement limits (95% CI) are flagged and labelled in the report graphs. A few random outliers may appear in any data set, especially in larger data sets that may be associated with efficacy data. The user may identify a cause for the difference and either remove the data from the file or set the value in the "toExclude" column to TRUE and then resubmit the data for analysis. If the "toExclude" value is TRUE, the data will not be used to calculate the statistics, but will appear in the graphs.

![Efficacy Correlation plot.](Webtool%20Output/MSD20_JRW/MSD29Corr.png){#fig-effcorrplot}

@fig-effcorrplot is a correlation plot of the two data sets with a unity reference line and Spearman's correlation coefficient. The data should overlay the equality reference line.

The calculated data, plots, and statistics can be downloaded for documentation and future reference.

### Constant CV

Often the variability in efficacy data varies across the dynamic range of an assay and is better represented by the CV. Unfortunately this violates a primary assumption in the analysis, making the replicate-experiment less useful. This is illustrated in the following example, where the the CV is 10% across the assay range.

![Mean Difference plot for an assay with 10% CV.](Webtool%20Output/MSD_320_CV10/MSC_CV10_MD.png){#fig-effMDcv}

@fig-effMDcv shows the variability increasing as the efficacy values increase. This occurs in signal increase assays, where the raw data values for the vehicle controls are smaller than those for active samples. The pattern would be reversed for a signal decrease assay.

```{r efficacyCVStats, echo=FALSE}

MSDCVStats <- read_csv('Webtool Output/MSD_320_CV10/MSD_320_CV10_EFF_19817/MSD_320_CV10_EFF_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE)))

knitr::kable(MSDCVStats, 'pipe')
```

![Correlation plot for an assay with 10% CV.](Webtool%20Output/MSD_320_CV10/MSD_CV10Corr.png){#fig-effCorrcv}

The same pattern is observed in the correlation plot.

If an assay displays this behavior, consider binning compounds by activity (e.g. inactive, moderately active, highly active) and running the replicate experiment for each group of samples.

### Screening - Variability differences across assay range.

The variability of assay efficacy measurements may differ significantly across the dynamic range of the assay. Many assays may display a constant coefficient of variation (CV), instead of a constant standard deviation. This can be problematic in large screening data sets where the majority of inactive compounds may have a variance that is different from the few active compounds, as illustrated in the following example which contains 9,500 inactive samples and 500 active samples.

![Mean-Difference Plot Screen Efficacy Data.](Webtool%20Output/Screen/ScreenMD.png){#fig-scrnMD}

```{r ScreenMSD}

ScrnStats <- read_csv('Webtool Output/Screen/ScrnMSD_EFF_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE)))

knitr::kable(ScrnStats, 'pipe')

```

![Screen Efficacy Correlation Plot.](Webtool%20Output/Screen/ScreenCorr.png){#fig-scrCorr}

Both the Mean-Difference and Correlation plots show what appears to be 2 populations and more variability in the data at higher activity values.

This example is based on a signal increase assays, where the raw data values are higher for active compounds than for vehicle controls. In the case of a signal decrease assay the variability would be increased at lower % activity levels (larger raw data values).

Assays which show this behavior should not use the replicate experiment as a general measurement of uncertainty in the assay. However Replicate-Experiments with the vehicle and active controls analyzed separately can be useful to document that the assay is repeatable and helping to define limits for activity. In fact, the numerator in the Z' equation^5^ is the difference between the Limits of Agreement for the two controls.

$$
Z' = \frac{(\overline{Max} - 3 * sd_{Max})-(\overline{Min} + 3 * sd_{Min})}{\overline{Max} - \overline{Min}}
$$

## Potency

Potency data are the concentration of a substance which elicits a specified efficacy. These are generated from concentration response curves. AC~50~ values are used for the examples, but any potency measurement (K~i~, LD~80~, etc.) can be used.

A replicate-experiment can be used to determine the minimum ratio in measured potency for 2 samples to be statistically different ( $p \leq 0.05$). This is known as the **Minimum Significant Ratio (MSR)**. Ideally at least 20 compounds are tested in two experiments. The experiments may be run on different days or in parallel, as long as the reagents/cells are preparede independently. The dose curves for the recommended 20-30 compounds can still be accommodated with 1-2 384-well plates/run. Ideally these compounds should have potency values which span the range of interest for the assay. The potency data are generally expressed in molar concentrations (e.g uM, nM, pM), though other units (eg. dilution factors, wt/volume) can be used, as long as the potency values are fit from a standard Hill equation (Y vs log(X).

### Log-normal Distribution {#sec-lognorm}

Potency values generally follow a log-normal distribution.^6^ In other words, log transformed potency values are normally distributed. Potency values are determined from the efficacy plotted at log(concentration), which is often represented as an x-axis with a log scale. Therefore when doing a statistical analysis, which assumes the data are normally distributed, the analysis must be done on log transformed data.

![Log-normal Distribution](Figs/LogNormFig.png){#fig-lognorm}

A simulated data set of 10,000 potency measurements of compound with a potency of 100 nM was used to illustrate some of the properties of log-normal data. @fig-lognorm shows several views of the data distribution. Panel A represents the original data values plotted on a linear axis. Panel B is the log~10~ transformed potency data and exhibits the expected normal distribution centered on 2 (log~10~(100) = 2). Panel C is. the original data plotted with log~10~ transformed axis which now appears to be normally distributed. Transforming the axis spacing for log-normal data facilitates visualiation of the variability in the data. The true potency of 100 is the geometric mean of the data set and would represent the median in the distribution of the original data. Note that the peak of the original data is less than the geometric mean, since the data is right skewed. Similarly this skew will result in arithmetic means do not represent the midpoint of the distribution.

This log-normal data distribution means that the data analysis must be performed on log(Potency) data as described in @sec-data_analysis. The differences and associated statistics are shown in @fig-diffhist. These are then converted back to the linear scale for reporting, so differences are transformed to ratios and a difference of 0 becomes a ratio of 1. These transformed values are also log-normal and the axis scales are transformed for graphing.

### Example

Below are the first 6 samples from an example experiment. The data is simply the sample identifiers (numeric or character) and the measured potency values for the 2 experiments. This is all that is needed to upload for analysis.

```{r MSR32Data}

MSR3Data <- read_csv('Data/MSR3data32.csv') 

knitr::kable(head(MSR3Data), 'pipe')
```

The data analysis is similar to that used with efficacy data, except that the potency values must first be transformed to their log~10~ values, so the variability will have a normal distribution for the statistical analysis. Then the means and differences for the pairs of log~10~(Potency) values are determined along with the associated statistics as described in #sec-data_analysis.

```{r MSR32CalcData}

MSR3CalcData <- read_csv('Webtool Output/MSR3/MSR3data32_POT_OutputData_tbl.csv') %>% 
  select(Sample, Exp1, Exp2, GeoMean, Ratio, Class) %>% 
  mutate(Sample = as.character(Sample),
         across(where(is.numeric), ~ format(.x, digits = 3, scientific = TRUE)))

knitr::kable(head(MSR3CalcData), 'pipe')

```

The GeoMean is the geometric mean of the potency values and the Ratio is the transformation of the difference from the statistical analysis. Recall that $log(A) - log(B) = log(A/B)$, so the difference between 2 log values becomes a ratio, when anti-logged. Similarly the mean of logs becomes a geometric mean when anti-logged.

***Note, while the analysis is done on the log scale, data are presented in the original linear scale.***

![Mean-Ratio plot of potency data.](Webtool%20Output/MSR3/MSR3MR.png){#fig-potMD}

The primary output of the analysis is the Mean Ratio plot. The geometric mean value of each data pair is plotted on the x-axis while the ratio between the potencies is represented on the y-axis. Both axes are plotted on the log scale, as described in @sec-lognorm. Plotting the geometric mean on the x-axis and ratio on the y-axis provides an overview of the variability in the data across the activity range of the assay. This plot is similar to a residuals plot. If the data were identical and the variability was simply random, then the data points should be centered around 1 with about half of the data on either side of the center. The statistics in the plot are based on the distribution in @fig-diffstats.

The blue lines show the mean ratio (solid) and the ratio limits (dashed) as well as a reference line at 1. The reference line is the expected ratio, if the variability is random. The ratio limits represent a 95% CI around the mean ratio. If this includes 1, then the variability in the data is random with no significant systematic bias between the two runs. If the reference line does not fall within the ratio limits, this suggests a difference between the runs due to a cause (e.g. reagents, equipment, samples, environment, ...).

The plot also includes agreement limits around the mean ratio. Any two potency measurements, both within and between samples, are considered to be significantly different if the ratio of the larger value to the smaller value is greater than the MSR. These statistics are derived from the log(Potency) difference values as described in @sec-data_analysis.

```{r potMSR3stats, echo=FALSE}

MSR3Stats <- read_csv('Webtool Output/MSR3/MSR3data32_POT_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE)))

knitr::kable(MSR3Stats, 'pipe')
```

![Potency Correlation plot.](Webtool%20Output/MSR3/MSR3Corr.png){#fig-potCorr}

@fig-potCorr is a correlation plot of the two data sets with a unity reference line and the Spearman correlation. The data should overlay the equality reference line.

The calculated data, plots, and statistics can be downloaded for documentation and future reference.

### Too Few Samples

Early in a project, it may be a struggle to identify a sufficient number of validated, active samples to perform a replicate experiment. Here is an example where there are only 5 unique samples. Unfortunately this low sample number does not provide enough statistical power to obtain a good estimate of the assays variability, so the Ratio limits and MSR can appear to be greater than their true values.

![Mean Ratio plot.](Webtool%20Output/FewSmplRep_POT_19774/FewSmplMR.png){#fig-fewMD}

```{r fewSmplStats, echo=FALSE}

MSRfewStats <- read_csv('Webtool Output/FewSmplRep_POT_19774/FewSmpl_POT_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE))) 

knitr::kable(MSRfewStats, 'pipe')
```

![Correlation Plot](Webtool%20Output/FewSmplRep_POT_19774/FewSmplCorr.png){#fig-fewCorr}

Fortunately there is a simple solution to this problem. Each sample can be replicated multiple times on the plates and the replicate dose response curves analyzed as if they were from different samples. Ideally, these replicate samples should be prepared as if they were unique compounds with their own dilution series. In this example, the 5 unique samples were each replicated 6 times (e.g. sample 1 becomes 1A, 1B, 1C ...). This produces 30 pairs of potency values for the analysis

![Replicated samples Mean Ratio plot,](Webtool%20Output/FewSmplRep_POT_19774/FewSmplRep.MR.png)

```{r MSRRepSmplStats, echo=FALSE}

MSRRepSmplStats <- read_csv('Webtool Output/FewSmplRep_POT_19774/FewSmplRep_POT_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE))) 

knitr::kable(MSRRepSmplStats, 'pipe')
```

![Replicated samples correlation plot,](Webtool%20Output/FewSmplRep_POT_19774/FewSmplRepCorr.png){#fig-repCorr}

While it is easy to identify the clusters due to the sample replication in the graphs, the improvement in statistical power from n=30 instead of n = 5 provides a much better estimate of the true assay variability. Both the MSR and the ratio limits are significantly lower.

### Systematic Difference between Runs (change example to larger difference)

When the mean ratio line is displaced from the ratio = 1 reference line, this may be an indication of a systematic difference between the two experiments. However if the reference line is within the ratio limits, this could simply be random variation, or due to a cause with a very small effect. If the reference line is outside of the ratio limits, this can indicate a systematic difference which should be investigated.

![Systematic error Mean Ratio plot.](Webtool%20Output/MSR3data32shift_POT_19774/MSR3ShiftMR.png){#fig-seMD}

```{r PotShiftStats, echo=FALSE}

PotShiftStats <- read_csv('Webtool Output/MSR3data32shift_POT_19774/MSR3data32shift_POT_Stats_tbl.csv') %>% 
  mutate(across(where(is.numeric), ~ format(.x, digits = 3, scientific = FALSE))) 

knitr::kable(PotShiftStats, 'pipe')
```

![Systematic error Correlation plot.](Webtool%20Output/MSR3data32shift_POT_19774/MSR3ShiftCorr.png){#fig-seCorr}

Here we can see an example where the reference line lies outside of the ratio limits. The correlation line is also slightly different from the unity line.

Common causes can include any of the following:

1.  Differences in the samples between experiments (e.g. sample lots, or solubilizations).

2.  Reagents or cells used in the assay.

3.  Equipment changes.

4.  Environmental changes (e.g. temperature).

5.  Personnel

If a cause is identified, it should me mitigated (if possible) and the replicate experiment repeated. Sometimes a cause may be identified which includes historical data that can't be changed. If this happens, it may be significant enough to require retesting of key compounds to assess the impact.

## References

1.  Bland, J. M., and D. G. Altman. “Statistical Methods for Assessing Agreement between Two Methods of Clinical Measurement.” *Lancet (London, England)* 1, no. 8476 (February 8, 1986): 307–10.

2.  Iversen, Philip W., Benoit Beck, Yun-Fei Chen, Walthere Dere, Viswanath Devanarayan, Brian J. Eastwood, Mark W. Farmen, et al. “HTS Assay Validation.” In *Assay Guidance Manual*, edited by Sarine Markossian, Abigail Grossman, Kyle Brimacombe, Michelle Arkin, Douglas Auld, Chris Austin, Jonathan Baell, et al. Bethesda (MD): Eli Lilly & Company and the National Center for Advancing Translational Sciences, 2004. http://www.ncbi.nlm.nih.gov/books/NBK83783/.

3.  Haas, Joseph V., Brian J. Eastwood, Philip W. Iversen, Viswanath Devanarayan, and Jeffrey R. Weidner. “Minimum Significant Ratio – A Statistic to Assess Assay Variability.” In *Assay Guidance Manual*, edited by Sarine Markossian, Abigail Grossman, Kyle Brimacombe, Michelle Arkin, Douglas Auld, Chris Austin, Jonathan Baell, et al. Bethesda (MD): Eli Lilly & Company and the National Center for Advancing Translational Sciences, 2004. http://www.ncbi.nlm.nih.gov/books/NBK169432/.

4.  Dahlin, Jayme L., G. Sitta Sittampalam, Nathan P. Coussens, Viswanath Devanarayan, Jeffrey R. Weidner, Philip W. Iversen, Joseph V. Haas, et al. “Basic Guidelines for Reporting Non-Clinical Data.” In *Assay Guidance Manual*, edited by Sarine Markossian, Abigail Grossman, Michelle Arkin, Douglas Auld, Chris Austin, Jonathan Baell, Kyle Brimacombe, et al. Bethesda (MD): Eli Lilly & Company and the National Center for Advancing Translational Sciences, 2004. http://www.ncbi.nlm.nih.gov/books/NBK550206/

5.  Zhang, Ji-Hu, Thomas D.Y. Chung, and Kevin R. Oldenburg. “A Simple Statistical Parameter for Use in Evaluation and Validation of High Throughput Screening Assays.” *SLAS Discovery* 4, no. 2 (April 1999): 67–73. https://doi.org/10.1177/108705719900400206.

6.  Elassaiss-Schaap, Jeroen, and Kevin Duisters. “Variability in the Log Domain and Limitations to Its Approximation by the Normal Distribution.” *CPT: Pharmacometrics & Systems Pharmacology* 9, no. 5 (May 2020): 245–57. https://doi.org/10.1002/psp4.12507.
